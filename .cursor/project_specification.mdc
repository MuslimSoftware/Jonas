# Jonas AI Agent Development Guide (Based on Specification v1.0)

**Objective:** This document serves as your primary guide and specification for building the Jonas AI Codebase Automation Agent. Refer back to this document consistently throughout the development process to ensure your implementation aligns with the project's goals, architecture, workflow, and constraints.

**Your Role:** As the AI assistant building Jonas, you are responsible for implementing the features and components described herein, adhering strictly to the specified architecture, workflow, and critical considerations.

---

## 1. Core Mission & Goals

**Purpose:** You are building Jonas, an AI agent designed to automate software development tasks. Its main function is to implement and test code changes within a target codebase, using external context (like Trello cards, Google Docs, database records, internal tool outputs) and user requests.

**Primary Goals:**
*   **Accelerate Development:** Reduce the time required for implementing context-specific fixes or features (e.g., related to user bookings).
*   **Improve Consistency:** Ensure a standardized approach to applying these changes.
*   **Reduce Repetitive Tasks:** Automate routine coding and testing steps for human developers.

**Core Functionality Summary:** Jonas must be able to:
1.  Receive a task (from various sources).
2.  Gather context (APIs, DBs, GUI tools via Agent S2).
3.  Analyze the relevant codebase.
4.  Generate and implement code changes.
5.  Execute tests against the changes.
6.  Create a Git branch for review.

---

## 2. Architectural Blueprint: Modular Design

**Requirement:** Implement Jonas using a modular architecture orchestrated by a central backend controller. This promotes separation of concerns and allows for easier maintenance and extension.

**Key Components You Will Build/Integrate:**

*   **Orchestrator (Core Backend):**
    *   **Language/Framework:** Use Python (FastAPI or Flask recommended).
    *   **Responsibilities:**
        *   Manage the agent's state machine (task lifecycle: PENDING, GATHERING_CONTEXT, etc.).
        *   Coordinate the execution of different modules/tools based on the workflow phase.
        *   Handle user requests received via WebSockets from the frontend.
        *   Manage task persistence (store state in a database like PostgreSQL or MongoDB).
*   **Specialized Modules/Tools (Integrate or Wrap):**
    *   **Task Input Parser:**
        *   Must handle inputs from Trello (API), Google Docs (API), and potentially raw text.
        *   Use NLP/LLM capabilities to extract key details (task description, booking IDs).
    *   **Database Connector:**
        *   Use standard Python libraries (e.g., `psycopg2`, `SQLAlchemy`) for **direct** database interaction (querying booking data). **Do not use GUI-based SQL clients.**
    *   **Agent S2 Interface Module:**
        *   **Purpose:** This is **specifically** for interacting with Graphical User Interfaces (GUIs) when direct APIs or command-line access is **unavailable** (e.g., scraping internal web tools, GUI-based testing steps).
        *   **Implementation:** Create a dedicated service/wrapper that allows the Orchestrator to delegate GUI tasks to an Agent S2 instance (running separately, likely on a VM with a desktop environment).
        *   **Interface:** Define clear inputs (target URL/app, task description, locators) and outputs (scraped data, status, logs/screenshots).
    *   **Code Analysis Module:**
        *   Integrate with code-aware LLMs/tools (e.g., Cursor API, Sourcegraph Cody, AugmentCode, or a suitable fine-tuned model) to identify relevant code sections based on task context.
    *   **Code Implementation Module:**
        *   Integrate with code generation/editing LLMs/tools (e.g., Aider, LLM APIs) to apply planned changes **directly to files**. **Do not simulate typing code via Agent S2.**
    *   **Testing Execution Module:**
        *   Wrap `subprocess` or similar to execute command-line test runners (e.g., `pytest`, `npm test`).
        *   Must parse test output to determine success or failure.
    *   **Git Module:**
        *   Wrap `subprocess` to execute standard Git commands (`clone`, `checkout`, `add`, `commit`, `push`). **Do not use GUI Git clients.**
*   **State Management:** Implement a robust system within the Orchestrator to track the state of each task, using a database for persistence.
*   **Communication Layer:** Implement a WebSocket server (e.g., using FastAPI WebSockets, Flask-SocketIO) tightly integrated with the Orchestrator for real-time, bidirectional communication with the frontend UI.

---

## 3. Mandated Workflow: Phased Execution

**Requirement:** Jonas **must** follow this specific, phased workflow for each task. Adhere to the primary methods described and only use fallbacks or conditional methods (like Agent S2) when explicitly indicated.

**Phase 1: Context Gathering & Planning**

*   **1-a: Task & Environment Context Gathering:**
    *   **Input:** Task Source (Trello URL/ID, Google Doc URL, text).
    *   **Step 1 (Parse Task):** Prioritize Trello/Google Docs APIs. Use LLM for parsing. **Fallback:** If APIs fail, use the `Agent S2 Interface Module` to scrape the source via GUI.
    *   **Step 2 (Fetch Booking Data):** Use the `Database Connector` module for **direct** SQL queries. **Agent S2 Role: None.**
    *   **Step 3 (Fetch Debug Info/Internal Tools):** **Primary:** If the tool is GUI-based and has no API, use the `Agent S2 Interface Module` to navigate and scrape. **Alternative:** If an API exists, use direct calls (`requests`).
    *   **Output:** A structured context object (JSON/dict). Send updates via WebSocket.
*   **1-b: Code Context Analysis & Implementation Planning:**
    *   **Input:** Structured context from 1-a, codebase access.
    *   **Step 1 (Codebase Analysis):** Use the `Code Analysis Module` (LLM/tool integration) with the gathered context. **Agent S2 Role: None.**
    *   **Step 2 (Develop Plan):** Use the planning capabilities of the `Code Analysis` or `Code Implementation` module. Synthesize context and analysis into a step-by-step code modification plan. **Agent S2 Role: None.**
    *   **Output:** A detailed coding plan. Send updates via WebSocket. Display in UI.

**Phase 2: Automated Code Implementation**

*   **Input:** Detailed coding plan, codebase write access.
*   **Step 1 (Code Mod):** Use the `Code Implementation Module` (LLM/tool integration) to apply changes **directly to files.** **Agent S2 Role: None.**
*   **Output:** Modified code files. Send status updates via WebSocket.

**Phase 3: Automated Code Testing & Branching**

*   **Input:** Modified code, original task context.
*   **Step 1 (Setup Test Env):** Prioritize direct methods (`Database Connector` for SQL, API calls). **Conditional:** Only use `Agent S2 Interface Module` if essential setup steps *require* GUI interaction (e.g., a 'clone booking' button in a GUI tool).
*   **Step 2 (Run Tests):** **Primary:** Use `Testing Execution Module` for CLI test suites. **Conditional:** Only use `Agent S2 Interface Module` if tests *require* GUI interaction (e.g., specific E2E tests, visual validation).
*   **Step 3 (Analyze Results):** Parse output from Step 2 (CLI or Agent S2).
*   **Step 4 (Greenlight/Revise):** Determine pass/fail. Log errors clearly.
*   **Step 5 (Git Branch - on success):** **Primary:** Use `Git Module` for `git` commands. **Fallback (Highly Discouraged):** Avoid using Agent S2 for typing Git commands.
*   **Output:** Test results, Git branch name. Send final updates via WebSocket. Display in UI.

---

## 4. User Interface Requirements

**Requirement:** Implement a web-based frontend (React, Vue, or Angular recommended) communicating with the backend via WebSockets.

**Key UI Components:**

*   **Three-Panel Layout:**
    *   **Left:** Chat History / Task List (Select tasks, start new).
    *   **Center:** Chat Interface (User input, Jonas messages/updates, Markdown support, processing indicators).
    *   **Right:** Context & Status (Tabbed Interface):
        *   **Tab 1 (Live Status):** Real-time display of Jonas's current action (including specific indication for Agent S2 activity), activity indicator, timestamped action log.
        *   **Tab 2 (Gathered Context):** Display structured context from Phase 1-a (Task summary, IDs, DB records, Tool info). Use collapsible sections.
        *   **Tab 3 (Plan & Results):** Display coding plan (Phase 1-b), final test results (Phase 3), and Git branch name.
*   **Real-time Updates:** Use WebSockets extensively to push all status changes, logs, context, and messages from the backend Orchestrator to the frontend *without* page reloads.

---

## 5. Backend & Infrastructure Guidelines

*   **Backend:** Python 3.x (FastAPI/Flask).
*   **Database:** PostgreSQL or MongoDB (for task state, history, context cache).
*   **WebSockets:** Use a robust library (FastAPI WebSockets, Flask-SocketIO).
*   **Agent S2 Deployment:** Plan for Agent S2 to run on a separate machine/VM with a GUI desktop environment and required applications. The Orchestrator must be able to communicate with it (likely via an API/RPC layer you may need to help define or implement around Agent S2).
*   **Codebase Access:** Ensure secure SSH key or token management for Git operations.
*   **Hosting:** Consider standard cloud platforms (AWS, GCP, Azure) or on-premise servers. Account for compute instances (Orchestrator), database service, a GUI VM (for Agent S2), and potentially GPU instances if using local LLMs.
*   **Resource Management:** Design the Orchestrator to handle concurrent tasks and potentially manage Agent S2 instance pooling if needed.

---

## 6. Critical Implementation Principles (Mandatory Considerations)

**Requirement:** Adhere strictly to these principles during development:

*   **Agent S2 is a Tool of Last Resort:**
    *   Recognize that GUI automation via Agent S2 is **inherently brittle** and prone to failures due to UI changes, timing issues, etc.
    *   **Prioritize direct methods** (APIs, database connections, command-line tools) whenever possible.
    *   Use Agent S2 **only** for tasks where no reliable alternative exists (specific GUI scraping, essential GUI-based test steps).
    *   Implement **robust error detection, retry mechanisms, and clear logging** within the `Agent S2 Interface Module`. Ensure failures are reported clearly back to the Orchestrator.
*   **Prioritize Security:**
    *   Grant Jonas the **least privilege** necessary for its tasks.
    *   Implement **secure credential management** (use Vault, cloud secrets managers, or environment variables – avoid hardcoding secrets).
    *   Consider network segmentation between components.
    *   Be aware of the risks of executing LLM-generated code; implement safeguards if possible. **Human review of created branches is essential.**
*   **Implement Comprehensive Error Handling & Recovery:**
    *   Design strategies for handling failures at **every step** of the workflow (API errors, DB timeouts, code compilation errors, test failures, Agent S2 exceptions, Git conflicts).
    *   Define how Jonas should react (e.g., retry, pause and wait for user input, fail gracefully).
    *   Ensure all failures are clearly communicated to the user via the frontend UI and logs.
*   **Mitigate LLM Challenges:**
    *   Be aware that LLM outputs (analysis, plans, code) can be inconsistent or incorrect (hallucinations).
    *   Employ good prompt engineering practices.
    *   Implement validation steps where possible (e.g., syntax checking generated code).
    *   Rely on the testing phase to catch functional errors in generated code.
*   **Design for Scalability:**
    *   Consider potential bottlenecks (LLM API rate limits, Agent S2 instance availability, backend resources).
    *   Design components with potential future scaling in mind.
*   **Start Simple:**
    *   Focus initial development on well-defined, relatively simple task types. Handling complex logic, large refactorings, or merge conflicts automatically is a future enhancement, not a V1 requirement.
*   **Manage Context Windows:**
    *   If dealing with large codebases or extensive context, implement strategies like context summarization, chunking, or embedding-based retrieval to stay within LLM limits.
*   **Be Mindful of Costs:** Consider the operational costs (LLM APIs, hosting, especially GUI VMs/GPUs).

---

## 7. Future Vision (Context Only)

While not for immediate implementation, be aware of potential future enhancements:
*   Interactive refinement of plans/code.
*   Learning from feedback.
*   Advanced Git operations (rebasing, conflict handling).
*   Code review integration.
*   Expanded input sources (Slack, Jira).
*   Test generation.
*   Multi-agent collaboration.

---

**Proceed with development, keeping this guide as your central reference point.**